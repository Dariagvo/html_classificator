{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30635,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install wandb\n",
    "!pip install 'transformers[torch]' \n",
    "!pip install datasets \n",
    "!pip install evaluate"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-02-23T17:45:04.924239Z",
     "iopub.execute_input": "2024-02-23T17:45:04.924646Z",
     "iopub.status.idle": "2024-02-23T17:45:56.658731Z",
     "shell.execute_reply.started": "2024-02-23T17:45:04.924604Z",
     "shell.execute_reply": "2024-02-23T17:45:56.657545Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.2)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.32)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.39.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (68.1.2)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.11.17)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.36.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.12.2)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.24.3)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.8.8)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.0)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch!=1.12.0,>=1.10 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.0.0)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.25.0)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.12.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch!=1.12.0,>=1.10->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2023.11.17)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch!=1.12.0,>=1.10->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch!=1.12.0,>=1.10->transformers[torch]) (1.3.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.24.3)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2023.12.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.8.5)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.1.0)\nRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (3.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nCollecting evaluate\n  Obtaining dependency information for evaluate from https://files.pythonhosted.org/packages/70/63/7644a1eb7b0297e585a6adec98ed9e575309bb973c33b394dae66bc35c69/evaluate-0.4.1-py3-none-any.whl.metadata\n  Downloading evaluate-0.4.1-py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.24.3)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.7)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.0.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.15)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.12.2)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.20.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (11.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.5)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.2.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.11.17)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m1.1 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m0m\n\u001B[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.1\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AlbertForSequenceClassification, AlbertConfig, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "from transformers import default_data_collator\n",
    "\n",
    "import evaluate\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T17:45:56.661217Z",
     "iopub.execute_input": "2024-02-23T17:45:56.661649Z",
     "iopub.status.idle": "2024-02-23T17:45:59.444151Z",
     "shell.execute_reply.started": "2024-02-23T17:45:56.661609Z",
     "shell.execute_reply": "2024-02-23T17:45:59.443251Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "TEST_SIZE = 0.3\n",
    "SPLIT_RANDOM_SEED = 42"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T17:45:59.445248Z",
     "iopub.execute_input": "2024-02-23T17:45:59.445543Z",
     "iopub.status.idle": "2024-02-23T17:45:59.449619Z",
     "shell.execute_reply.started": "2024-02-23T17:45:59.445514Z",
     "shell.execute_reply": "2024-02-23T17:45:59.448605Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def encode(examples):\n",
    "    result = tokenizer(examples[\"text\"], truncation=True, max_length=512, padding=\"max_length\")\n",
    "    return result\n",
    "\n",
    "@torch.no_grad()\n",
    "def test(model, loader, device, tqdm_desc):\n",
    "    loss_log = []\n",
    "    acc_log = []\n",
    "    model.eval()\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "\n",
    "    for input_ids, attention_mask, labels in loader:\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        out = model(input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_func(out.logits, labels)\n",
    "\n",
    "        loss_log.append(loss.item())\n",
    "\n",
    "        pred = torch.argmax(out.logits, dim=1)\n",
    "        acc_log.append((pred == labels).detach().cpu().numpy().sum() / len(pred))\n",
    "\n",
    "    return loss_log, acc_log\n",
    "\n",
    "\n",
    "def train(model, optimizer, n_epochs, train_loader, val_loader, batch_size, scheduler=None):\n",
    "    train_loss = []\n",
    "    train_acc = []\n",
    "    \n",
    "    run = wandb.init(project='html classificator', reinit=True)\n",
    "    wandb.watch(model, nn.CrossEntropyLoss(), log=\"all\", log_freq=100)\n",
    "    model.train()\n",
    "\n",
    "    batch = 0\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    for epoch in range(n_epochs):\n",
    "        for input_ids, attention_mask, labels in tqdm(train_loader, desc=f'Training {epoch}/{n_epochs}'):\n",
    "            batch += 1\n",
    "            \n",
    "            input_ids = input_ids.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            out = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "            loss = loss_func(out.logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss.append(loss.item())\n",
    "\n",
    "            pred = torch.argmax(out.logits, dim=1)\n",
    "            train_acc.append((pred == labels).detach().cpu().numpy().sum() / len(pred))\n",
    "            \n",
    "            if batch == 100:\n",
    "                batch = 0\n",
    "                \n",
    "                val_loss, val_acc = test(model, val_loader, device, tqdm_desc='Validating')\n",
    "                model.train()\n",
    "                \n",
    "                wandb.log({\"train\": {\"acc\": np.mean(train_acc), \"loss\": np.mean(train_loss)}, \"val\": {\"acc\": np.mean(val_acc), \"loss\": np.mean(val_loss)}})\n",
    "                \n",
    "                # clear_output()\n",
    "                print(f\"Next 100 batches:\")\n",
    "                print(f\" train loss: {np.mean(train_loss)}, train acc: {np.mean(train_acc)}\")\n",
    "                print(f\" val loss: {np.mean(val_loss)}, val acc: {np.mean(val_acc)}\\n\")\n",
    "                train_loss = []\n",
    "                train_acc = []\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "    # последние батчи\n",
    "    val_loss, val_acc = test(model, val_loader, device, tqdm_desc='Validating')\n",
    "    wandb.log({\"train\": {\"acc\": np.mean(train_acc), \"loss\": np.mean(train_loss)}, \"val\": {\"acc\": np.mean(val_acc), \"loss\": np.mean(val_loss)}})\n",
    "    print(f\"Last batches:\")\n",
    "    print(f\" train loss: {np.mean(train_loss)}, train acc: {np.mean(train_acc)}\")\n",
    "    print(f\" val loss: {np.mean(val_loss)}, val acc: {np.mean(val_acc)}\\n\")\n",
    "\n",
    "    wandb.unwatch()\n",
    "    run.finish()\n",
    "    return train_loss_log, train_acc_log, val_loss_log, val_acc_log"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T17:48:54.555651Z",
     "iopub.execute_input": "2024-02-23T17:48:54.556360Z",
     "iopub.status.idle": "2024-02-23T17:48:54.574714Z",
     "shell.execute_reply.started": "2024-02-23T17:48:54.556333Z",
     "shell.execute_reply": "2024-02-23T17:48:54.573810Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T17:45:59.476858Z",
     "iopub.execute_input": "2024-02-23T17:45:59.477119Z",
     "iopub.status.idle": "2024-02-23T17:45:59.542758Z",
     "shell.execute_reply.started": "2024-02-23T17:45:59.477094Z",
     "shell.execute_reply": "2024-02-23T17:45:59.541832Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "text": "cuda:0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(\"XSY/albert-base-v2-imdb-calssification\")\n",
    "# model = AlbertForSequenceClassification(AlbertConfig()).cuda()\n",
    "\n",
    "# Load model directly\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"albert/albert-base-v2\")\n",
    "model = AlbertForSequenceClassification.from_pretrained(\"albert/albert-base-v2\").to(device)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"XSY/albert-base-v2-imdb-calssification\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"XSY/albert-base-v2-imdb-calssification\").to(device)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T17:45:59.543953Z",
     "iopub.execute_input": "2024-02-23T17:45:59.544237Z",
     "iopub.status.idle": "2024-02-23T17:46:02.457695Z",
     "shell.execute_reply.started": "2024-02-23T17:45:59.544212Z",
     "shell.execute_reply": "2024-02-23T17:46:02.456828Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6e2057e6213848669d51da3209ca74eb"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff1995fe25344083aedf0091d929e1f6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "spiece.model:   0%|          | 0.00/760k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "df412e1cf24142e4a818dbbcbee7de1f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.31M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4c2607d413664b8bbd98ffd93e387153"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/47.4M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5739fa7ed18e4a00b547b7c2961a0855"
      }
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Some weights of AlbertForSequenceClassification were not initialized from the model checkpoint at albert/albert-base-v2 and are newly initialized: ['classifier.weight', 'classifier.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3f1c85a496654695b643a71eca7e7114"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = load_dataset(\"imdb\")\n",
    "tokenized_datasets = dataset.map(encode, batched=True, remove_columns=\"text\")\n",
    "\n",
    "input_ids_t, input_ids_v, attention_mask_t, attention_mask_v, label_t, label_v = train_test_split(torch.tensor(tokenized_datasets['train']['input_ids']), torch.tensor(tokenized_datasets['train']['attention_mask']), torch.tensor(tokenized_datasets['train']['label']), \n",
    "                 random_state=SPLIT_RANDOM_SEED, test_size=TEST_SIZE, shuffle=True)\n",
    "\n",
    "train_dataset = TensorDataset(input_ids_t, attention_mask_t, label_t)\n",
    "val_dataset = TensorDataset(input_ids_v, attention_mask_v, label_v)\n",
    "test_dataset = TensorDataset(torch.tensor(tokenized_datasets['test']['input_ids']), torch.tensor(tokenized_datasets['test']['attention_mask']), torch.tensor(tokenized_datasets['test']['label']))\n",
    "\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T17:46:02.459017Z",
     "iopub.execute_input": "2024-02-23T17:46:02.459773Z",
     "iopub.status.idle": "2024-02-23T17:48:52.626810Z",
     "shell.execute_reply.started": "2024-02-23T17:46:02.459736Z",
     "shell.execute_reply": "2024-02-23T17:48:52.625806Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/1.79k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "255e986caa534153b9d2d00a4d1bcd42"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading metadata:   0%|          | 0.00/1.05k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "72fe2cffd6534d5891a9ad9010bc9a6f"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.02 MiB, post-processed: Unknown size, total: 207.25 MiB) to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1...\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/84.1M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "85e99d9c4f564674bf7597f54ef87bc6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "Dataset imdb downloaded and prepared to /root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1. Subsequent calls will reuse this data.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "259d801d758749319c57cad9a6f8443f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/25 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56c0e791c4cd442abed4f03d8eb655d1"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/25 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "98ebb31e3acb41438c4c5867c8fe85ad"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/50 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "21b2dbdf873a44cab1d337487b1f3506"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_datasets"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-07T14:30:05.652060Z",
     "iopub.execute_input": "2024-02-07T14:30:05.652472Z",
     "iopub.status.idle": "2024-02-07T14:30:05.660255Z",
     "shell.execute_reply.started": "2024-02-07T14:30:05.652441Z",
     "shell.execute_reply": "2024-02-07T14:30:05.659289Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": [
    {
     "execution_count": 8,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 25000\n    })\n    test: Dataset({\n        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 25000\n    })\n    unsupervised: Dataset({\n        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n        num_rows: 50000\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "wandb.login(key=\"eba16103be2afd0b5c96243771d60f5d7e562f68\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T17:48:52.628066Z",
     "iopub.execute_input": "2024-02-23T17:48:52.628383Z",
     "iopub.status.idle": "2024-02-23T17:48:54.552839Z",
     "shell.execute_reply.started": "2024-02-23T17:48:52.628357Z",
     "shell.execute_reply": "2024-02-23T17:48:54.551915Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: W&B API key is configured. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001B[34m\u001B[1mwandb\u001B[0m: \u001B[33mWARNING\u001B[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001B[34m\u001B[1mwandb\u001B[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
     "output_type": "stream"
    },
    {
     "execution_count": 9,
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# показания модели imdb (преобученная)\n",
    "# model = model.to(device)\n",
    "val_loss, val_acc = test(model, val_loader, device, tqdm_desc='Test')\n",
    "print(f'Без обучения на датасете (чисто модель с параметрами) loss = {np.mean(val_loss)}, accuracy = {np.mean(val_acc)}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-08T11:16:01.663487Z",
     "iopub.execute_input": "2024-02-08T11:16:01.664218Z",
     "iopub.status.idle": "2024-02-08T11:31:45.287849Z",
     "shell.execute_reply.started": "2024-02-08T11:16:01.664183Z",
     "shell.execute_reply": "2024-02-08T11:31:45.286797Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": [
    {
     "name": "stderr",
     "text": "Test: 100%|██████████| 1563/1563 [15:43<00:00,  1.66it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Без обучения на датасете (чисто модель с параметрами) loss = 0.1985105358434201, accuracy = 0.9360204734484965\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T09:10:22.626012Z",
     "iopub.execute_input": "2024-02-23T09:10:22.626747Z",
     "iopub.status.idle": "2024-02-23T09:10:22.937768Z",
     "shell.execute_reply.started": "2024-02-23T09:10:22.626715Z",
     "shell.execute_reply": "2024-02-23T09:10:22.935276Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "execution_count": 12,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=3e-10)\n",
    "train_loss_log, train_acc_log, val_loss_log, val_acc_log = train(model, optimizer, 1, train_loader, val_loader, batch_size, scheduler)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T17:50:02.121784Z",
     "iopub.execute_input": "2024-02-23T17:50:02.122129Z",
     "iopub.status.idle": "2024-02-23T19:06:54.616402Z",
     "shell.execute_reply.started": "2024-02-23T17:50:02.122103Z",
     "shell.execute_reply": "2024-02-23T19:06:54.614805Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mrodion-chernomordin\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240223_175002-pb7178th</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/pb7178th' target=\"_blank\">virtuous-horse-26</a></strong> to <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator</a>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/pb7178th' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/pb7178th</a>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Training 0/1:   9%|▉         | 100/1094 [06:56<22:26:35, 81.28s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Next 100 batches:\n train loss: 0.40254384763538836, train acc: 0.829375\n val loss: 0.2803314509295197, val acc: 0.898676261549396\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 0/1:  18%|█▊        | 200/1094 [13:53<20:10:56, 81.27s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Next 100 batches:\n train loss: 0.27852990984916687, train acc: 0.895\n val loss: 0.32157292686450456, val acc: 0.8734452736318408\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 0/1:  27%|██▋       | 300/1094 [20:50<17:56:28, 81.35s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Next 100 batches:\n train loss: 0.2514940486475825, train acc: 0.903125\n val loss: 0.22795579036765262, val acc: 0.9184434968017058\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 0/1:  37%|███▋      | 400/1094 [27:46<15:40:08, 81.28s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Next 100 batches:\n train loss: 0.24557729426771402, train acc: 0.90625\n val loss: 0.26205476823010676, val acc: 0.8992537313432836\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 0/1:  46%|████▌     | 500/1094 [34:43<13:24:47, 81.29s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Next 100 batches:\n train loss: 0.22316523604094982, train acc: 0.91125\n val loss: 0.2117769074107983, val acc: 0.9211975835110164\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 0/1:  55%|█████▍    | 600/1094 [41:41<11:09:41, 81.34s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Next 100 batches:\n train loss: 0.2266567359957844, train acc: 0.91\n val loss: 0.21679778061886587, val acc: 0.9191098081023454\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 0/1:  64%|██████▍   | 700/1094 [48:38<8:54:10, 81.35s/it] ",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Next 100 batches:\n train loss: 0.2382584900967777, train acc: 0.915625\n val loss: 0.2278970120383351, val acc: 0.9107587064676617\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 0/1:  73%|███████▎  | 800/1094 [55:35<6:38:38, 81.36s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Next 100 batches:\n train loss: 0.20685897704213857, train acc: 0.920625\n val loss: 0.205471274842109, val acc: 0.9266613361762616\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 0/1:  82%|████████▏ | 900/1094 [1:02:32<4:22:57, 81.33s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Next 100 batches:\n train loss: 0.2258072620816529, train acc: 0.915625\n val loss: 0.19755653966305606, val acc: 0.9251066098081023\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 0/1:  91%|█████████▏| 1000/1094 [1:09:29<2:07:27, 81.36s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Next 100 batches:\n train loss: 0.244444946013391, train acc: 0.9\n val loss: 0.22660695995364996, val acc: 0.912091329068941\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 0/1: 100%|██████████| 1094/1094 [1:11:51<00:00,  3.94s/it]  \n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Last batches:\n train loss: 0.2126205377280712, train acc: 0.9188829787234043\n val loss: 0.2024343078956008, val acc: 0.923818407960199\n\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">virtuous-horse-26</strong> at: <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/pb7178th' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/pb7178th</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240223_175002-pb7178th/logs</code>"
     },
     "metadata": {}
    },
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m optimizer \u001B[38;5;241m=\u001B[39m optim\u001B[38;5;241m.\u001B[39mAdam(model\u001B[38;5;241m.\u001B[39mparameters(), lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2e-5\u001B[39m, weight_decay\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.01\u001B[39m)\n\u001B[1;32m      2\u001B[0m scheduler \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39moptim\u001B[38;5;241m.\u001B[39mlr_scheduler\u001B[38;5;241m.\u001B[39mCosineAnnealingLR(optimizer, T_max\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, eta_min\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3e-10\u001B[39m)\n\u001B[0;32m----> 3\u001B[0m train_loss_log, train_acc_log, val_loss_log, val_acc_log \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscheduler\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[0;32mIn[10], line 85\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(model, optimizer, n_epochs, train_loader, val_loader, batch_size, scheduler)\u001B[0m\n\u001B[1;32m     83\u001B[0m wandb\u001B[38;5;241m.\u001B[39munwatch()\n\u001B[1;32m     84\u001B[0m run\u001B[38;5;241m.\u001B[39mfinish()\n\u001B[0;32m---> 85\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrain_loss_log\u001B[49m, train_acc_log, val_loss_log, val_acc_log\n",
      "\u001B[0;31mNameError\u001B[0m: name 'train_loss_log' is not defined"
     ],
     "ename": "NameError",
     "evalue": "name 'train_loss_log' is not defined",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = test(model, test_loader, device, tqdm_desc='Test')\n",
    "print(f'loss = {np.mean(test_loss)}, accuracy = {np.mean(test_acc)}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-23T19:06:54.841367Z",
     "iopub.execute_input": "2024-02-23T19:06:54.841989Z",
     "iopub.status.idle": "2024-02-23T19:21:37.584176Z",
     "shell.execute_reply.started": "2024-02-23T19:06:54.841948Z",
     "shell.execute_reply": "2024-02-23T19:21:37.583275Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "text": "loss = 0.1889111399695263, accuracy = 0.9254638515674984\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# та же модель, но логирование раз в эпоху\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=5, eta_min=3e-10)\n",
    "train_loss_log, train_acc_log, val_loss_log, val_acc_log = train(model, optimizer, 5, train_loader, val_loader, batch_size, scheduler)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-16T17:36:27.387436Z",
     "iopub.execute_input": "2024-02-16T17:36:27.388066Z",
     "iopub.status.idle": "2024-02-16T20:52:17.553027Z",
     "shell.execute_reply.started": "2024-02-16T17:36:27.388038Z",
     "shell.execute_reply": "2024-02-16T20:52:17.552270Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "text": "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mrodion-chernomordin\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.2"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/kaggle/working/wandb/run-20240216_173627-488xlbnz</code>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/488xlbnz' target=\"_blank\">luminous-bao-23</a></strong> to <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator</a>"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/488xlbnz' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/488xlbnz</a>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Training 0/5: 100%|██████████| 1094/1094 [32:46<00:00,  1.80s/it]\nValidating 0/5: 100%|██████████| 469/469 [06:18<00:00,  1.24it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 1\n train loss: 0.24377623107028917, train acc: 0.9031649908592322\n val loss: 0.23198189499027438, val acc: 0.9111140724946695\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 1/5: 100%|██████████| 1094/1094 [32:49<00:00,  1.80s/it]\nValidating 1/5: 100%|██████████| 469/469 [06:15<00:00,  1.25it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 2\n train loss: 0.15352085591302383, train acc: 0.9429273308957953\n val loss: 0.19266452457207733, val acc: 0.9263059701492538\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 2/5: 100%|██████████| 1094/1094 [32:45<00:00,  1.80s/it]\nValidating 2/5: 100%|██████████| 469/469 [06:15<00:00,  1.25it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 3\n train loss: 0.10570448501566783, train acc: 0.963989183424741\n val loss: 0.19919305919572267, val acc: 0.9287046908315565\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 3/5: 100%|██████████| 1094/1094 [32:46<00:00,  1.80s/it]\nValidating 3/5: 100%|██████████| 469/469 [06:15<00:00,  1.25it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 4\n train loss: 0.04956031080909581, train acc: 0.9848606032906764\n val loss: 0.21317417312042117, val acc: 0.93363539445629\n\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "Training 4/5: 100%|██████████| 1094/1094 [32:44<00:00,  1.80s/it]\nValidating 4/5: 100%|██████████| 469/469 [06:15<00:00,  1.25it/s]\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Epoch 5\n train loss: 0.020495343316119536, train acc: 0.9954296160877514\n val loss: 0.2581087981553268, val acc: 0.933590973702914\n\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "VBox(children=(Label(value='0.226 MB of 0.226 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": ""
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">luminous-bao-23</strong> at: <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/488xlbnz' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/488xlbnz</a><br/>Synced 6 W&B file(s), 10 media file(s), 10 artifact file(s) and 0 other file(s)"
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240216_173627-488xlbnz/logs</code>"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "test_loss, test_acc = test(model, test_loader, device, tqdm_desc='Test')\n",
    "print(f'loss = {np.mean(test_loss)}, accuracy = {np.mean(test_acc)}')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-02-16T20:52:17.554570Z",
     "iopub.execute_input": "2024-02-16T20:52:17.554886Z",
     "iopub.status.idle": "2024-02-16T21:07:55.490570Z",
     "shell.execute_reply.started": "2024-02-16T20:52:17.554843Z",
     "shell.execute_reply": "2024-02-16T21:07:55.489683Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "text": "Test: 100%|██████████| 1563/1563 [15:37<00:00,  1.67it/s]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "loss = 0.24936352291772165, accuracy = 0.9337811900191939\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  }
 ]
}
