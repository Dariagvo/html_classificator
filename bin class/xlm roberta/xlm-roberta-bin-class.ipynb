{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7851698,"sourceType":"datasetVersion","datasetId":4604683}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install wandb\n!pip install 'transformers[torch]'\n!pip install datasets ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-05T19:46:04.525725Z","iopub.execute_input":"2024-04-05T19:46:04.526115Z","iopub.status.idle":"2024-04-05T19:46:46.039327Z","shell.execute_reply.started":"2024-04-05T19:46:04.526063Z","shell.execute_reply":"2024-04-05T19:46:46.038141Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.16.3)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.7)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.41)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.40.5)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0.1)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.3)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (69.0.3)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.11)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2024.2.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\nRequirement already satisfied: transformers[torch] in /opt/conda/lib/python3.10/site-packages (4.38.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.20.3)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.31.0)\nRequirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.15.2)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.4.2)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (4.66.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (2.1.2)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from transformers[torch]) (0.27.2)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.3)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2024.2.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers[torch]) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->transformers[torch]) (3.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers[torch]) (2024.2.2)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->transformers[torch]) (2.1.3)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from datasets) (4.66.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.2.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.20.3)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from datasets) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.13.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->datasets) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import wandb\nimport torch\nimport pandas as pd\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport random\n\nimport numpy as np\n\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, precision_recall_curve, confusion_matrix\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, default_data_collator, XLMRobertaForSequenceClassification\n\nfrom torch.utils.data import DataLoader, TensorDataset\n\nfrom tqdm import tqdm\n\nfrom IPython.display import clear_output\nfrom datasets import load_dataset, Dataset\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:46:46.041281Z","iopub.execute_input":"2024-04-05T19:46:46.041609Z","iopub.status.idle":"2024-04-05T19:47:17.012269Z","shell.execute_reply.started":"2024-04-05T19:46:46.041579Z","shell.execute_reply":"2024-04-05T19:47:17.011479Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-04-05 19:47:02.409444: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-05 19:47:02.409587: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-05 19:47:02.681655: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"def calc_metrics(y_true, y_pred):\n    y_true = y_true.cpu()\n    y_pred = y_pred.cpu()\n    \n    res = confusion_matrix(y_true, y_pred).ravel()\n    if len(res) == 1:\n        if y_true[0] == 0:\n            tn, fp, fn, tp = res[0], 0, 0, 0\n        else:\n            tn, fp, fn, tp = 0, 0, 0, res[0]\n    else:\n        tn, fp, fn, tp = res\n    \n    return accuracy_score(y_true, y_pred), precision_score(y_true, y_pred, zero_division=0), recall_score(y_true, y_pred, zero_division=0), f1_score(y_true, y_pred, zero_division=0), tn, fp, fn, tp\n\n@torch.no_grad()\ndef test(model, loader, device, tqdm_desc):\n    loss_log = []\n    acc_log = []\n    prec_log = []\n    rec_log = []\n    f1_log = []\n    \n    tn_log = []\n    fp_log = []\n    fn_log = []\n    tp_log = []\n    \n    model.eval()\n    loss_func = nn.CrossEntropyLoss(weight=torch.tensor([0.73, 0.27]).to(device))\n\n    for input_ids, attention_mask, labels in loader:\n        input_ids = input_ids.to(device)\n        attention_mask = attention_mask.to(device)\n        labels = labels.to(device)\n\n        out = model(input_ids, attention_mask=attention_mask)\n        loss = loss_func(out.logits, labels)\n\n        loss_log.append(loss.item())\n\n        pred = torch.argmax(out.logits, dim=1)\n        res = calc_metrics(labels, pred)\n        acc_log.append(res[0])\n        prec_log.append(res[1])\n        rec_log.append(res[2])\n        f1_log.append(res[3])\n        tn_log.append(res[4])\n        fp_log.append(res[5])\n        fn_log.append(res[6])\n        tp_log.append(res[7])\n\n    return loss_log, acc_log, prec_log, rec_log, f1_log, tn_log, fp_log, fn_log, tp_log\n\n\ndef train(model, optimizer, n_epochs, train_loader, val_loader, batch_size, scheduler=None, log_batch_count=500, preval=True):    \n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    \n    train_loss = []\n    train_acc = []\n    train_prec = []\n    train_rec = []\n    train_f1 = []\n    \n    train_tn = []\n    train_fp = []\n    train_fn = []\n    train_tp = []\n    \n    run = wandb.init(project='html classificator', reinit=True)\n    wandb.watch(model, nn.CrossEntropyLoss(weight=torch.tensor([0.73, 0.27]).to(device)), log=\"all\", log_freq=100)\n    model.train()\n\n    batch = 0\n    loss_func = nn.CrossEntropyLoss(weight=torch.tensor([0.73, 0.27]).to(device))\n    \n    \n    if preval:\n        print(f\"Init loss:\")\n        val_loss, val_acc, val_prec, val_rec, val_f1, val_tn, val_fp, val_fn, val_tp = test(model, val_loader, device, tqdm_desc='Validating')\n        model.train()\n\n        wandb.log({\"val\": {\"acc\": np.mean(val_acc), \"pre\": np.mean(val_prec), \"rec\": np.mean(val_rec), \"f1\": np.mean(val_f1), \"loss\": np.mean(val_loss), \n                           \"true neg\": np.mean(val_tn), \"false pos\": np.mean(val_fp), \"false neg\": np.mean(val_fn), \"true pos\": np.mean(val_tp)}})\n        \n        print(f\" val loss: {np.mean(val_loss)}, val acc: {np.mean(val_acc)}\\n\")\n    \n    for epoch in range(n_epochs):\n        for input_ids, attention_mask, labels in tqdm(train_loader, desc=f'Training {epoch}/{n_epochs}'):\n            batch += 1\n            \n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            out = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = loss_func(out.logits, labels)\n            loss.backward()\n            optimizer.step()\n            \n            if batch <= 5:\n                proba = F.softmax(out.logits, dim=-1)\n                proba = proba.cpu()\n                wandb.log({\"proba\": wandb.Histogram(proba.detach().numpy())})\n\n            train_loss.append(loss.item())\n\n            pred = torch.argmax(out.logits, dim=1)\n            res = calc_metrics(labels, pred)\n            train_acc.append(res[0])\n            train_prec.append(res[1])\n            train_rec.append(res[2])\n            train_f1.append(res[3])\n            train_tn.append(res[4])\n            train_fp.append(res[5])\n            train_fn.append(res[6])\n            train_tp.append(res[7])\n            \n            if batch == log_batch_count:\n                batch = 0\n                \n                val_loss, val_acc, val_prec, val_rec, val_f1, val_tn, val_fp, val_fn, val_tp = test(model, val_loader, device, tqdm_desc='Validating')\n                model.train()\n                \n                wandb.log({\"train\": {\"acc\": np.mean(train_acc), \"pre\": np.mean(train_prec), \"rec\": np.mean(train_rec), \"f1\": np.mean(train_f1), \"loss\": np.mean(train_loss), \n                                     \"true neg\": np.mean(train_tn), \"false pos\": np.mean(train_fp), \"false neg\": np.mean(train_fn), \"true pos\": np.mean(train_tp)}, \n                           \"val\": {\"acc\": np.mean(val_acc), \"pre\": np.mean(val_prec), \"rec\": np.mean(val_rec), \"f1\": np.mean(val_f1), \"loss\": np.mean(val_loss), \n                                   \"true neg\": np.mean(val_tn), \"false pos\": np.mean(val_fp), \"false neg\": np.mean(val_fn), \"true pos\": np.mean(val_tp)}})\n                \n                # clear_output()\n#                 print(f\"Next 100 batches:\")\n#                 print(f\" train loss: {np.mean(train_loss)}, train acc: {np.mean(train_acc)}\")\n#                 print(f\" val loss: {np.mean(val_loss)}, val acc: {np.mean(val_acc)}\\n\")\n                train_loss = []\n                train_acc = []\n                train_prec = []\n                train_rec = []\n                train_f1 = []\n                train_tn = []\n                train_fp = []\n                train_fn = []\n                train_tp = []\n\n        if scheduler is not None:\n            scheduler.step()\n            \n    # последние батчи\n    val_loss, val_acc, val_prec, val_rec, val_f1, val_tn, val_fp, val_fn, val_tp = test(model, val_loader, device, tqdm_desc='Validating')\n                \n    wandb.log({\"train\": {\"acc\": np.mean(train_acc), \"pre\": np.mean(train_prec), \"rec\": np.mean(train_rec), \"f1\": np.mean(train_f1), \"loss\": np.mean(train_loss), \n                         \"true neg\": np.mean(train_tn), \"false pos\": np.mean(train_fp), \"false neg\": np.mean(train_fn), \"true pos\": np.mean(train_tp)}, \n                \"val\": {\"acc\": np.mean(val_acc), \"pre\": np.mean(val_prec), \"rec\": np.mean(val_rec), \"f1\": np.mean(val_f1), \"loss\": np.mean(val_loss), \n                        \"true neg\": np.mean(val_tn), \"false pos\": np.mean(val_fp), \"false neg\": np.mean(val_fn), \"true pos\": np.mean(val_tp)}})\n    print(f\"Last batches:\")\n    print(f\" train loss: {np.mean(train_loss)}, train acc: {np.mean(train_acc)}\")\n    print(f\" val loss: {np.mean(val_loss)}, val acc: {np.mean(val_acc)}\\n\")\n\n    wandb.unwatch()\n    run.finish()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:47:17.013606Z","iopub.execute_input":"2024-04-05T19:47:17.014251Z","iopub.status.idle":"2024-04-05T19:47:17.054398Z","shell.execute_reply.started":"2024-04-05T19:47:17.014210Z","shell.execute_reply":"2024-04-05T19:47:17.053399Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:47:17.057138Z","iopub.execute_input":"2024-04-05T19:47:17.057780Z","iopub.status.idle":"2024-04-05T19:47:17.170510Z","shell.execute_reply.started":"2024-04-05T19:47:17.057742Z","shell.execute_reply":"2024-04-05T19:47:17.169485Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cuda:0\n","output_type":"stream"}]},{"cell_type":"code","source":"wandb.login(key=\"eba16103be2afd0b5c96243771d60f5d7e562f68\")","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:47:17.172099Z","iopub.execute_input":"2024-04-05T19:47:17.172988Z","iopub.status.idle":"2024-04-05T19:47:19.608161Z","shell.execute_reply.started":"2024-04-05T19:47:17.172944Z","shell.execute_reply":"2024-04-05T19:47:19.607107Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Обрабатываем данные**","metadata":{}},{"cell_type":"code","source":"table_path = \"/kaggle/input/site-bin-class/pars_data.csv\"\ndata = pd.read_csv(table_path, on_bad_lines='skip', sep=';', lineterminator='\\n')\ndata.dropna(axis=0, how='any', inplace=True)\ndata.text = data.text.astype(str)\n\ndataset = Dataset.from_pandas(data)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:47:19.609330Z","iopub.execute_input":"2024-04-05T19:47:19.609954Z","iopub.status.idle":"2024-04-05T19:47:36.549191Z","shell.execute_reply.started":"2024-04-05T19:47:19.609927Z","shell.execute_reply":"2024-04-05T19:47:36.548306Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-04T14:48:32.004983Z","iopub.execute_input":"2024-04-04T14:48:32.005330Z","iopub.status.idle":"2024-04-04T14:48:32.018254Z","shell.execute_reply.started":"2024-04-04T14:48:32.005302Z","shell.execute_reply":"2024-04-04T14:48:32.017272Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                                text  target\n0  индивидуальные групповые занятия будни выходны...       1\n1  royal life федеральное театральное агентство «...       0\n2  автоматизированная система uontravel войти заб...       0\n3  уходовая домашняя косметика для души тела зада...       0\n4  новости обучение трейдингу нуля обучение трейд...       1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>индивидуальные групповые занятия будни выходны...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>royal life федеральное театральное агентство «...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>автоматизированная система uontravel войти заб...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>уходовая домашняя косметика для души тела зада...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>новости обучение трейдингу нуля обучение трейд...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"print('общий размер датасета', data.shape[0])\nprint('количество элементов не из образования', data[data['target'] == 0]['text'].count())\nprint('количество элементов из образования', data[data['target'] == 1]['text'].count())\nprint('максимальный размер строки в датасете', max(len(i) for i in data['text']))","metadata":{"execution":{"iopub.status.busy":"2024-04-04T14:48:35.497672Z","iopub.execute_input":"2024-04-04T14:48:35.498295Z","iopub.status.idle":"2024-04-04T14:48:35.531787Z","shell.execute_reply.started":"2024-04-04T14:48:35.498264Z","shell.execute_reply":"2024-04-04T14:48:35.530801Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"общий размер датасета 47621\nколичество элементов не из образования 34656\nколичество элементов из образования 12965\nмаксимальный размер строки в датасете 1339641\n","output_type":"stream"}]},{"cell_type":"code","source":"# гистог по лог от размера строки\n\nplt.hist(np.log([len(i) for i in data['text']]))\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-04T14:50:04.308535Z","iopub.execute_input":"2024-04-04T14:50:04.308915Z","iopub.status.idle":"2024-04-04T14:50:04.525837Z","shell.execute_reply.started":"2024-04-04T14:50:04.308885Z","shell.execute_reply":"2024-04-04T14:50:04.524820Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"np.exp(10)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T14:53:28.330022Z","iopub.execute_input":"2024-04-04T14:53:28.330741Z","iopub.status.idle":"2024-04-04T14:53:28.337560Z","shell.execute_reply.started":"2024-04-04T14:53:28.330704Z","shell.execute_reply":"2024-04-04T14:53:28.336414Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"22026.465794806718"},"metadata":{}}]},{"cell_type":"markdown","source":"# **Инициализируем модель и токенайзер (обрабатываем текст)**","metadata":{}},{"cell_type":"code","source":"# Load model directly\n\ntokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-base\")\nmodel = XLMRobertaForSequenceClassification.from_pretrained(\"FacebookAI/xlm-roberta-base\").to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:56:33.386026Z","iopub.execute_input":"2024-04-05T19:56:33.386439Z","iopub.status.idle":"2024-04-05T19:56:35.951706Z","shell.execute_reply.started":"2024-04-05T19:56:33.386408Z","shell.execute_reply":"2024-04-05T19:56:35.950701Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at FacebookAI/xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\nTEST_SIZE = 0.3\nSPLIT_RANDOM_SEED = 42\nMAX_LENGTH = 512\n\ndef encode(examples):\n    result = tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n    return result\n\ntokenized_datasets = dataset.map(encode, batched=True, remove_columns=\"text\")","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:47:46.407603Z","iopub.execute_input":"2024-04-05T19:47:46.407901Z","iopub.status.idle":"2024-04-05T19:50:11.516032Z","shell.execute_reply.started":"2024-04-05T19:47:46.407875Z","shell.execute_reply":"2024-04-05T19:50:11.514914Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ab7a6a3b29f2460aa113221bc2bb18be"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 8min 17s, sys: 1.85 s, total: 8min 19s\nWall time: 2min 25s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n\ninput_ids_train, input_ids_v, attention_mask_train, attention_mask_v, label_train, label_v = train_test_split(torch.tensor(tokenized_datasets['input_ids']), \n                                                                                                  torch.tensor(tokenized_datasets['attention_mask']), \n                                                                                                  torch.tensor(tokenized_datasets['target']), \n                                                                                                  random_state=SPLIT_RANDOM_SEED, test_size=TEST_SIZE, shuffle=True)\n\ninput_ids_val, input_ids_test, attention_mask_val, attention_mask_test, label_val, label_test = train_test_split(input_ids_v, attention_mask_v, label_v, \n                                                                                                  random_state=SPLIT_RANDOM_SEED, test_size=0.33, shuffle=True)\n\ntrain_dataset = TensorDataset(input_ids_train, attention_mask_train, label_train)\nval_dataset = TensorDataset(input_ids_val, attention_mask_val, label_val)\ntest_dataset = TensorDataset(input_ids_test, attention_mask_test, label_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:53:47.463850Z","iopub.execute_input":"2024-04-05T14:53:47.464228Z","iopub.status.idle":"2024-04-05T14:54:34.928893Z","shell.execute_reply.started":"2024-04-05T14:53:47.464195Z","shell.execute_reply":"2024-04-05T14:54:34.927895Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"CPU times: user 47 s, sys: 780 ms, total: 47.8 s\nWall time: 47.5 s\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:54:34.930183Z","iopub.execute_input":"2024-04-05T14:54:34.930577Z","iopub.status.idle":"2024-04-05T14:54:34.936042Z","shell.execute_reply.started":"2024-04-05T14:54:34.930543Z","shell.execute_reply":"2024-04-05T14:54:34.935126Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# **Базовая модель (512 размер строки)**","metadata":{}},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=3e-10)\ntrain(model, optimizer, 3, train_loader, val_loader, batch_size, scheduler, 500, True)","metadata":{"execution":{"iopub.status.busy":"2024-03-15T16:13:55.377223Z","iopub.execute_input":"2024-03-15T16:13:55.377792Z","iopub.status.idle":"2024-03-15T20:23:06.139862Z","shell.execute_reply.started":"2024-03-15T16:13:55.377762Z","shell.execute_reply":"2024-03-15T20:23:06.139048Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrodion-chernomordin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.4 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240315_161355-3pdqqtr8</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3pdqqtr8' target=\"_blank\">jumping-sun-34</a></strong> to <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3pdqqtr8' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3pdqqtr8</a>"},"metadata":{}},{"name":"stderr","text":"Training 0/3:  24%|██▍       | 500/2084 [19:33<47:37:06, 108.22s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.3438436353430152, train acc: 0.84425\n val loss: 0.25169663431623546, val acc: 0.8907554257095158\n\n","output_type":"stream"},{"name":"stderr","text":"Training 0/3:  48%|████▊     | 1000/2084 [39:11<32:32:22, 108.07s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.2522452085465193, train acc: 0.8975\n val loss: 0.25980170067082564, val acc: 0.8956594323873122\n\n","output_type":"stream"},{"name":"stderr","text":"Training 0/3:  72%|███████▏  | 1500/2084 [58:50<17:32:11, 108.10s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.2665684026386589, train acc: 0.884\n val loss: 0.24996682507093243, val acc: 0.8904424040066778\n\n","output_type":"stream"},{"name":"stderr","text":"Training 0/3:  96%|█████████▌| 2000/2084 [1:18:28<2:31:26, 108.17s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.2653320294730365, train acc: 0.88825\n val loss: 0.43475175078317696, val acc: 0.8143781302170284\n\n","output_type":"stream"},{"name":"stderr","text":"Training 0/3: 100%|██████████| 2084/2084 [1:20:46<00:00,  2.33s/it]   \nTraining 1/3:  20%|█▉        | 416/2084 [17:19<50:08:09, 108.21s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.25335297762602565, train acc: 0.890375\n val loss: 0.24690760237449322, val acc: 0.899728714524207\n\n","output_type":"stream"},{"name":"stderr","text":"Training 1/3:  44%|████▍     | 916/2084 [36:58<35:04:58, 108.13s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.25718214244768023, train acc: 0.892125\n val loss: 0.23911151059052732, val acc: 0.898059265442404\n\n","output_type":"stream"},{"name":"stderr","text":"Training 1/3:  68%|██████▊   | 1416/2084 [56:39<20:10:49, 108.76s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.2524174031764269, train acc: 0.8965\n val loss: 0.2548285537509742, val acc: 0.8944073455759599\n\n","output_type":"stream"},{"name":"stderr","text":"Training 1/3:  92%|█████████▏| 1916/2084 [1:16:19<5:03:03, 108.24s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.2632292756102979, train acc: 0.884625\n val loss: 0.2556516764940498, val acc: 0.8961811352253757\n\n","output_type":"stream"},{"name":"stderr","text":"Training 1/3: 100%|██████████| 2084/2084 [1:20:56<00:00,  2.33s/it]   \nTraining 2/3:  16%|█▌        | 332/2084 [15:00<52:39:04, 108.19s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.2546376099102199, train acc: 0.8899166666666666\n val loss: 0.23969308298410777, val acc: 0.8973288814691152\n\n","output_type":"stream"},{"name":"stderr","text":"Training 2/3:  40%|███▉      | 832/2084 [34:40<37:38:38, 108.24s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.24277572084218263, train acc: 0.895625\n val loss: 0.24984589096419949, val acc: 0.9010851419031719\n\n","output_type":"stream"},{"name":"stderr","text":"Training 2/3:  64%|██████▍   | 1332/2084 [54:19<22:37:33, 108.32s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.24453319256380202, train acc: 0.89575\n val loss: 0.23990811820364308, val acc: 0.8995200333889817\n\n","output_type":"stream"},{"name":"stderr","text":"Training 2/3:  88%|████████▊ | 1832/2084 [1:13:57<7:34:22, 108.18s/it]","output_type":"stream"},{"name":"stdout","text":"Next 100 batches:\n train loss: 0.24176935820281506, train acc: 0.897375\n val loss: 0.2338708041232695, val acc: 0.9017111853088481\n\n","output_type":"stream"},{"name":"stderr","text":"Training 2/3: 100%|██████████| 2084/2084 [1:20:55<00:00,  2.33s/it]   \n","output_type":"stream"},{"name":"stdout","text":"Last batches:\n train loss: 0.24218072983184977, train acc: 0.8946759259259259\n val loss: 0.23673500844241682, val acc: 0.9010851419031719\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">jumping-sun-34</strong> at: <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3pdqqtr8' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3pdqqtr8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240315_161355-3pdqqtr8/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"test_loss, test_acc, test_prec, test_rec, test_f1 = test(model, test_loader, device, tqdm_desc='Test')\nprint(f'loss = {np.mean(test_loss)}, accuracy = {np.mean(test_acc)}, precision = {np.mean(test_prec)}, recall = {np.mean(test_rec)}, f1_score = {np.mean(test_f1)}')","metadata":{"execution":{"iopub.status.busy":"2024-03-15T20:23:06.141830Z","iopub.execute_input":"2024-03-15T20:23:06.142114Z","iopub.status.idle":"2024-03-15T20:25:41.736666Z","shell.execute_reply.started":"2024-03-15T20:23:06.142087Z","shell.execute_reply":"2024-03-15T20:25:41.735530Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"loss = 0.22772546339211827, accuracy = 0.9044491525423729, precision = 0.8426836158192089, recall = 0.7995466774280334, f1_score = 0.798695709903665\n","output_type":"stream"}]},{"cell_type":"markdown","source":"[xlm-roberta-base-binclass](https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3pdqqtr8?nw=nwuserrodionchernomordin)","metadata":{}},{"cell_type":"markdown","source":"# **Заморозим все параметры базовой robert (512 размер строки)**","metadata":{}},{"cell_type":"code","source":"for param in model.roberta.parameters():\n    param.requires_grad = False","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:35:38.085007Z","iopub.execute_input":"2024-04-04T16:35:38.085421Z","iopub.status.idle":"2024-04-04T16:35:38.093316Z","shell.execute_reply.started":"2024-04-04T16:35:38.085391Z","shell.execute_reply":"2024-04-04T16:35:38.092211Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"%%time\n\noptimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=3e-10)\ntrain(model, optimizer, 3, train_loader, val_loader, batch_size, scheduler, 500, True)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T16:35:41.599883Z","iopub.execute_input":"2024-04-04T16:35:41.600850Z","iopub.status.idle":"2024-04-04T19:04:06.125229Z","shell.execute_reply.started":"2024-04-04T16:35:41.600816Z","shell.execute_reply":"2024-04-04T19:04:06.124230Z"},"trusted":true},"execution_count":62,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:xil6fzql) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">azure-dew-41</strong> at: <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/xil6fzql' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/xil6fzql</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240404_163231-xil6fzql/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:xil6fzql). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240404_163541-u099nudh</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/u099nudh' target=\"_blank\">lucky-water-42</a></strong> to <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/u099nudh' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/u099nudh</a>"},"metadata":{}},{"name":"stdout","text":"Init loss:\n val loss: 0.606423469686349, val acc: 0.7298622704507512\n\n","output_type":"stream"},{"name":"stderr","text":"Training 0/3: 100%|██████████| 2084/2084 [45:23<00:00,  1.31s/it]    \nTraining 1/3: 100%|██████████| 2084/2084 [45:19<00:00,  1.30s/it]    \nTraining 2/3: 100%|██████████| 2084/2084 [45:19<00:00,  1.31s/it]    \n","output_type":"stream"},{"name":"stdout","text":"Last batches:\n train loss: 0.5201510729061233, train acc: 0.7455357142857143\n val loss: 0.5301524343494581, val acc: 0.7298622704507512\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">lucky-water-42</strong> at: <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/u099nudh' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/u099nudh</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240404_163541-u099nudh/logs</code>"},"metadata":{}},{"name":"stdout","text":"CPU times: user 2h 35min 37s, sys: 5min 45s, total: 2h 41min 22s\nWall time: 2h 28min 24s\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_acc, test_prec, test_rec, test_f1, _, _, _, _ = test(model, test_loader, device, tqdm_desc='Test')\nprint(f'loss = {np.mean(test_loss)}, accuracy = {np.mean(test_acc)}, precision = {np.mean(test_prec)}, recall = {np.mean(test_rec)}, f1_score = {np.mean(test_f1)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-04T19:09:21.633028Z","iopub.execute_input":"2024-04-04T19:09:21.633746Z","iopub.status.idle":"2024-04-04T19:11:55.827126Z","shell.execute_reply.started":"2024-04-04T19:09:21.633712Z","shell.execute_reply":"2024-04-04T19:11:55.826013Z"},"trusted":true},"execution_count":64,"outputs":[{"name":"stdout","text":"loss = 0.5321233773635605, accuracy = 0.7283127889060093, precision = 0.0, recall = 0.0, f1_score = 0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"[xlm-robert-binclass-freeze](https://wandb.ai/rodion-chernomordin/html%20classificator/runs/u099nudh?nw=nwuserrodionchernomordin)","metadata":{}},{"cell_type":"markdown","source":"# **Добавим веса в лосс**","metadata":{}},{"cell_type":"code","source":"optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=3e-10)\ntrain(model, optimizer, 3, train_loader, val_loader, batch_size, scheduler, 500, True)","metadata":{"execution":{"iopub.status.busy":"2024-04-04T19:29:30.195523Z","iopub.execute_input":"2024-04-04T19:29:30.196387Z","iopub.status.idle":"2024-04-05T00:05:31.849962Z","shell.execute_reply.started":"2024-04-04T19:29:30.196354Z","shell.execute_reply":"2024-04-05T00:05:31.849256Z"},"trusted":true},"execution_count":71,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Finishing last run (ID:kuf8fqdx) before initializing another..."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">zesty-blaze-44</strong> at: <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/kuf8fqdx' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/kuf8fqdx</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240404_192524-kuf8fqdx/logs</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Successfully finished last run (ID:kuf8fqdx). Initializing new run:<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240404_192930-ggln5kmp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/ggln5kmp' target=\"_blank\">neat-cloud-45</a></strong> to <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/ggln5kmp' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/ggln5kmp</a>"},"metadata":{}},{"name":"stdout","text":"Init loss:\n val loss: 0.6779123802415915, val acc: 0.7299666110183639\n\n","output_type":"stream"},{"name":"stderr","text":"Training 0/3: 100%|██████████| 2084/2084 [1:27:22<00:00,  2.52s/it]    \nTraining 1/3: 100%|██████████| 2084/2084 [1:27:24<00:00,  2.52s/it]    \nTraining 2/3: 100%|██████████| 2084/2084 [1:27:22<00:00,  2.52s/it]   \n","output_type":"stream"},{"name":"stdout","text":"Last batches:\n train loss: 0.19234290890883476, train acc: 0.8714451058201058\n val loss: 0.1711773048364907, val acc: 0.8857470784641068\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">neat-cloud-45</strong> at: <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/ggln5kmp' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/ggln5kmp</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240404_192930-ggln5kmp/logs</code>"},"metadata":{}}]},{"cell_type":"code","source":"test_loss, test_acc, test_prec, test_rec, test_f1, _, _, _, _ = test(model, test_loader, device, tqdm_desc='Test')\nprint(f'loss = {np.mean(test_loss)}, accuracy = {np.mean(test_acc)}, precision = {np.mean(test_prec)}, recall = {np.mean(test_rec)}, f1_score = {np.mean(test_f1)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-04T19:18:10.546176Z","iopub.status.idle":"2024-04-04T19:18:10.546706Z","shell.execute_reply.started":"2024-04-04T19:18:10.546435Z","shell.execute_reply":"2024-04-04T19:18:10.546462Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"[xlm-robert-binclass-weightloss](https://wandb.ai/rodion-chernomordin/html%20classificator/runs/ggln5kmp?nw=nwuserrodionchernomordin)","metadata":{}},{"cell_type":"markdown","source":"# **Логируем каждый батч, но одну эпоху (иначе умру) - полная модель с парам лоссом**","metadata":{}},{"cell_type":"code","source":"%%time\n\noptimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=3e-10)\ntrain(model, optimizer, 1, train_loader, val_loader, batch_size, scheduler, 1, True)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T13:37:51.274006Z","iopub.execute_input":"2024-04-05T13:37:51.274290Z","iopub.status.idle":"2024-04-05T14:48:09.049284Z","shell.execute_reply.started":"2024-04-05T13:37:51.274267Z","shell.execute_reply":"2024-04-05T14:48:09.048357Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrodion-chernomordin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240405_133751-p21a1qcp</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/p21a1qcp' target=\"_blank\">borg-space-46</a></strong> to <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/p21a1qcp' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/p21a1qcp</a>"},"metadata":{}},{"name":"stdout","text":"Init loss:\n val loss: 0.6361564451545626, val acc: 0.7298622704507512\n\n","output_type":"stream"},{"name":"stderr","text":"Training 0/1:   0%|          | 10/2084 [1:03:49<220:38:33, 382.99s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<timed exec>:3\u001b[0m\n","Cell \u001b[0;32mIn[3], line 123\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, n_epochs, train_loader, val_loader, batch_size, scheduler, log_batch_count, preval)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch \u001b[38;5;241m==\u001b[39m log_batch_count:\n\u001b[1;32m    121\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 123\u001b[0m     val_loss, val_acc, val_prec, val_rec, val_f1, val_tn, val_fp, val_fn, val_tp \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mValidating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    126\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(train_acc), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(train_prec), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(train_rec), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(train_f1), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(train_loss), \n\u001b[1;32m    127\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue neg\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(train_tn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse pos\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(train_fp), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse neg\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(train_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue pos\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(train_tp)}, \n\u001b[1;32m    128\u001b[0m                \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_acc), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_prec), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_rec), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_f1), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_loss), \n\u001b[1;32m    129\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue neg\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_tn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse pos\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_fp), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse neg\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue pos\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_tp)}})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[3], line 40\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, loader, device, tqdm_desc)\u001b[0m\n\u001b[1;32m     37\u001b[0m out \u001b[38;5;241m=\u001b[39m model(input_ids, attention_mask\u001b[38;5;241m=\u001b[39mattention_mask)\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(out\u001b[38;5;241m.\u001b[39mlogits, labels)\n\u001b[0;32m---> 40\u001b[0m loss_log\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     42\u001b[0m pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(out\u001b[38;5;241m.\u001b[39mlogits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     43\u001b[0m res \u001b[38;5;241m=\u001b[39m calc_metrics(labels, pred)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"[xlm-robert-binclass-f10b](https://wandb.ai/rodion-chernomordin/html%20classificator/runs/p21a1qcp?nw=nwuserrodionchernomordin)","metadata":{}},{"cell_type":"markdown","source":"# **Попробуем взять текст размера 2к**","metadata":{}},{"cell_type":"code","source":"%%time\n\nTEST_SIZE = 0.3\nSPLIT_RANDOM_SEED = 42\nMAX_LENGTH = 2000\n\ndef encode(examples):\n    result = tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n    return result\n\ntokenized_datasets = dataset.map(encode, batched=True, remove_columns=\"text\")","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:56:58.193782Z","iopub.execute_input":"2024-04-05T15:56:58.194097Z","iopub.status.idle":"2024-04-05T15:59:52.313216Z","shell.execute_reply.started":"2024-04-05T15:56:58.194073Z","shell.execute_reply":"2024-04-05T15:59:52.312236Z"},"trusted":true},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f321791e68314873a05c2cb3ba3702f1"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 8min 40s, sys: 3.94 s, total: 8min 44s\nWall time: 2min 54s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\n\ninput_ids_train, input_ids_v, attention_mask_train, attention_mask_v, label_train, label_v = train_test_split(torch.tensor(tokenized_datasets['input_ids']), \n                                                                                                  torch.tensor(tokenized_datasets['attention_mask']), \n                                                                                                  torch.tensor(tokenized_datasets['target']), \n                                                                                                  random_state=SPLIT_RANDOM_SEED, test_size=TEST_SIZE, shuffle=True)\n\ninput_ids_val, input_ids_test, attention_mask_val, attention_mask_test, label_val, label_test = train_test_split(input_ids_v, attention_mask_v, label_v, \n                                                                                                  random_state=SPLIT_RANDOM_SEED, test_size=0.33, shuffle=True)\n\ntrain_dataset = TensorDataset(input_ids_train, attention_mask_train, label_train)\nval_dataset = TensorDataset(input_ids_val, attention_mask_val, label_val)\ntest_dataset = TensorDataset(input_ids_test, attention_mask_test, label_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:59:52.314280Z","iopub.execute_input":"2024-04-05T15:59:52.314579Z","iopub.status.idle":"2024-04-05T16:02:55.518573Z","shell.execute_reply.started":"2024-04-05T15:59:52.314547Z","shell.execute_reply":"2024-04-05T16:02:55.517575Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"CPU times: user 3min 2s, sys: 2.98 s, total: 3min 5s\nWall time: 3min 3s\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:02:55.520732Z","iopub.execute_input":"2024-04-05T16:02:55.521194Z","iopub.status.idle":"2024-04-05T16:02:55.526433Z","shell.execute_reply.started":"2024-04-05T16:02:55.521159Z","shell.execute_reply":"2024-04-05T16:02:55.525724Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"%%time\n\noptimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=3e-10)\ntrain(model, optimizer, 3, train_loader, val_loader, batch_size, scheduler, 500, True)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T16:02:55.527474Z","iopub.execute_input":"2024-04-05T16:02:55.528106Z","iopub.status.idle":"2024-04-05T16:03:28.005341Z","shell.execute_reply.started":"2024-04-05T16:02:55.528080Z","shell.execute_reply":"2024-04-05T16:03:28.004147Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":11,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrodion-chernomordin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240405_160255-g2qc59em</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/g2qc59em' target=\"_blank\">andorian-quark-47</a></strong> to <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/g2qc59em' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/g2qc59em</a>"},"metadata":{}},{"name":"stdout","text":"Init loss:\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","File \u001b[0;32m<timed exec>:3\u001b[0m\n","Cell \u001b[0;32mIn[3], line 80\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, n_epochs, train_loader, val_loader, batch_size, scheduler, log_batch_count, preval)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preval:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInit loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 80\u001b[0m     val_loss, val_acc, val_prec, val_rec, val_f1, val_tn, val_fp, val_fn, val_tp \u001b[38;5;241m=\u001b[39m \u001b[43mtest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mValidating\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m     model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     83\u001b[0m     wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124macc\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_acc), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpre\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_prec), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrec\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_rec), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mf1\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_f1), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_loss), \n\u001b[1;32m     84\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue neg\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_tn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse pos\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_fp), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfalse neg\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_fn), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue pos\u001b[39m\u001b[38;5;124m\"\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(val_tp)}})\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[3], line 37\u001b[0m, in \u001b[0;36mtest\u001b[0;34m(model, loader, device, tqdm_desc)\u001b[0m\n\u001b[1;32m     34\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     35\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 37\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(out\u001b[38;5;241m.\u001b[39mlogits, labels)\n\u001b[1;32m     40\u001b[0m loss_log\u001b[38;5;241m.\u001b[39mappend(loss\u001b[38;5;241m.\u001b[39mitem())\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1568\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1565\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks, backward_pre_hooks)\n\u001b[1;32m   1566\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[0;32m-> 1568\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1570\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook_id, hook \u001b[38;5;129;01min\u001b[39;00m (\n\u001b[1;32m   1571\u001b[0m         \u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1572\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mitems(),\n\u001b[1;32m   1573\u001b[0m     ):\n\u001b[1;32m   1574\u001b[0m         \u001b[38;5;66;03m# mark that always called hook is run\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:1208\u001b[0m, in \u001b[0;36mXLMRobertaForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1200\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1202\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1203\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1204\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1206\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1208\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroberta\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1209\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1214\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1215\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1219\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1220\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py:803\u001b[0m, in \u001b[0;36mXLMRobertaModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken_type_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    802\u001b[0m     buffered_token_type_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings\u001b[38;5;241m.\u001b[39mtoken_type_ids[:, :seq_length]\n\u001b[0;32m--> 803\u001b[0m     buffered_token_type_ids_expanded \u001b[38;5;241m=\u001b[39m \u001b[43mbuffered_token_type_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexpand\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m     token_type_ids \u001b[38;5;241m=\u001b[39m buffered_token_type_ids_expanded\n\u001b[1;32m    805\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","\u001b[0;31mRuntimeError\u001b[0m: The expanded size of the tensor (2000) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [16, 2000].  Tensor sizes: [1, 514]"],"ename":"RuntimeError","evalue":"The expanded size of the tensor (2000) must match the existing size (514) at non-singleton dimension 1.  Target sizes: [16, 2000].  Tensor sizes: [1, 514]","output_type":"error"}]},{"cell_type":"markdown","source":"У нас может быть размер строки не больше 514 (2к не подойдут)","metadata":{}},{"cell_type":"markdown","source":"# **Посмотрим сколько по времени без всего обучается одна эпоха**","metadata":{}},{"cell_type":"code","source":"def only_train(model, optimizer, n_epochs, train_loader, val_loader, batch_size, scheduler=None, log_batch_count=500, preval=True):    \n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    \n    model.train()\n    batch = 0\n    loss_func = nn.CrossEntropyLoss(weight=torch.tensor([0.73, 0.27]).to(device))\n    \n    for epoch in range(n_epochs):\n        for input_ids, attention_mask, labels in tqdm(train_loader, desc=f'Training {epoch}/{n_epochs}'):\n            input_ids = input_ids.to(device)\n            attention_mask = attention_mask.to(device)\n            labels = labels.to(device)\n\n            optimizer.zero_grad()\n            out = model(input_ids, attention_mask=attention_mask, labels=labels)\n            loss = loss_func(out.logits, labels)\n            loss.backward()\n            optimizer.step()\n\n        if scheduler is not None:\n            scheduler.step()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:54:34.937384Z","iopub.execute_input":"2024-04-05T14:54:34.937850Z","iopub.status.idle":"2024-04-05T14:54:34.950156Z","shell.execute_reply.started":"2024-04-05T14:54:34.937820Z","shell.execute_reply":"2024-04-05T14:54:34.949286Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"%%time\n\noptimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=3e-10)\nonly_train(model, optimizer, 1, train_loader, val_loader, batch_size, scheduler, 500, True)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:54:34.952925Z","iopub.execute_input":"2024-04-05T14:54:34.953449Z","iopub.status.idle":"2024-04-05T15:50:05.345067Z","shell.execute_reply.started":"2024-04-05T14:54:34.953425Z","shell.execute_reply":"2024-04-05T15:50:05.344112Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"Training 0/3: 100%|██████████| 2084/2084 [54:34<00:00,  1.57s/it]\nTraining 1/3:   2%|▏         | 35/2084 [00:55<53:49,  1.58s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","File \u001b[0;32m<timed exec>:3\u001b[0m\n","Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36monly_train\u001b[0;34m(model, optimizer, n_epochs, train_loader, val_loader, batch_size, scheduler, log_batch_count, preval)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m input_ids, attention_mask, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 10\u001b[0m         input_ids \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m         attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     12\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"markdown","source":"54:34","metadata":{}},{"cell_type":"markdown","source":"# **Проверка на переобучение**","metadata":{}},{"cell_type":"code","source":"input_ids_train, input_ids_v, attention_mask_train, attention_mask_v, label_train, label_v = train_test_split(torch.tensor(tokenized_datasets['input_ids']), \n                                                                                                  torch.tensor(tokenized_datasets['attention_mask']), \n                                                                                                  torch.tensor(tokenized_datasets['target']), \n                                                                                                  random_state=SPLIT_RANDOM_SEED, test_size=TEST_SIZE, shuffle=True)\n\ninput_ids_val, input_ids_test, attention_mask_val, attention_mask_test, label_val, label_test = train_test_split(input_ids_v, attention_mask_v, label_v, \n                                                                                                  random_state=SPLIT_RANDOM_SEED, test_size=0.33, shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:01:21.003296Z","iopub.execute_input":"2024-04-05T17:01:21.003570Z","iopub.status.idle":"2024-04-05T17:02:08.681859Z","shell.execute_reply.started":"2024-04-05T17:01:21.003546Z","shell.execute_reply":"2024-04-05T17:02:08.681034Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"new_train_size = 1024\n\nprint(input_ids_train.shape)\ninput_ids_train, attention_mask_train, label_train = input_ids_train[:new_train_size], attention_mask_train[:new_train_size], label_train[:new_train_size]\nprint(input_ids_train.shape)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:02:08.685069Z","iopub.execute_input":"2024-04-05T17:02:08.685462Z","iopub.status.idle":"2024-04-05T17:02:08.696798Z","shell.execute_reply.started":"2024-04-05T17:02:08.685428Z","shell.execute_reply":"2024-04-05T17:02:08.695841Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"torch.Size([33334, 512])\ntorch.Size([1024, 512])\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataset = TensorDataset(input_ids_train, attention_mask_train, label_train)\nval_dataset = TensorDataset(input_ids_val, attention_mask_val, label_val)\ntest_dataset = TensorDataset(input_ids_test, attention_mask_test, label_test)\n\nbatch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:02:08.697922Z","iopub.execute_input":"2024-04-05T17:02:08.698192Z","iopub.status.idle":"2024-04-05T17:02:08.709918Z","shell.execute_reply.started":"2024-04-05T17:02:08.698170Z","shell.execute_reply":"2024-04-05T17:02:08.709200Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"%%time\n\noptimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=3e-10)\ntrain(model, optimizer, 3, train_loader, val_loader, batch_size, scheduler, 10, True)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T17:02:08.710927Z","iopub.execute_input":"2024-04-05T17:02:08.711229Z","iopub.status.idle":"2024-04-05T19:19:09.903373Z","shell.execute_reply.started":"2024-04-05T17:02:08.711196Z","shell.execute_reply":"2024-04-05T19:19:09.902485Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrodion-chernomordin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240405_170208-3szcvqqc</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3szcvqqc' target=\"_blank\">final-frontier-50</a></strong> to <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3szcvqqc' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3szcvqqc</a>"},"metadata":{}},{"name":"stdout","text":"Init loss:\n val loss: 0.6569885929956261, val acc: 0.7298622704507512\n\n","output_type":"stream"},{"name":"stderr","text":"Training 0/3: 100%|██████████| 64/64 [39:25<00:00, 36.97s/it]   \nTraining 1/3: 100%|██████████| 64/64 [39:25<00:00, 36.97s/it]   \nTraining 2/3: 100%|██████████| 64/64 [45:24<00:00, 42.56s/it]   \n","output_type":"stream"},{"name":"stdout","text":"Last batches:\n train loss: 0.3114948272705078, train acc: 0.59375\n val loss: 0.23224324491092677, val acc: 0.7298622704507512\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">final-frontier-50</strong> at: <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3szcvqqc' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3szcvqqc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240405_170208-3szcvqqc/logs</code>"},"metadata":{}},{"name":"stdout","text":"CPU times: user 2h 23min 2s, sys: 5min 24s, total: 2h 28min 26s\nWall time: 2h 17min 1s\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_acc, test_prec, test_rec, test_f1, _, _, _, _ = test(model, test_loader, device, tqdm_desc='Test')\nprint(f'loss = {np.mean(test_loss)}, accuracy = {np.mean(test_acc)}, precision = {np.mean(test_prec)}, recall = {np.mean(test_rec)}, f1_score = {np.mean(test_f1)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T19:19:09.905463Z","iopub.execute_input":"2024-04-05T19:19:09.905815Z","iopub.status.idle":"2024-04-05T19:21:55.041941Z","shell.execute_reply.started":"2024-04-05T19:19:09.905780Z","shell.execute_reply":"2024-04-05T19:21:55.040884Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"loss = 0.23473065966519258, accuracy = 0.7283127889060093, precision = 0.0, recall = 0.0, f1_score = 0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"[xlm-robert-binclass-overfitting](https://wandb.ai/rodion-chernomordin/html%20classificator/runs/3szcvqqc?nw=nwuserrodionchernomordin)","metadata":{}},{"cell_type":"markdown","source":"# **Проверка на здравие (ставим рандомные таргеты)**","metadata":{}},{"cell_type":"code","source":"table_path = \"/kaggle/input/site-bin-class/pars_data.csv\"\ndata = pd.read_csv(table_path, on_bad_lines='skip', sep=';', lineterminator='\\n')\ndata.dropna(axis=0, how='any', inplace=True)\ndata.text = data.text.astype(str)\n\nprint(data.target.tolist()[:100])\ntarget = data.target.tolist()\nnp.random.shuffle(target)\ndata.target = target\nprint(data.target.tolist()[:100])\n\ndataset = Dataset.from_pandas(data)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T20:09:25.711037Z","iopub.execute_input":"2024-04-05T20:09:25.711434Z","iopub.status.idle":"2024-04-05T20:09:36.245311Z","shell.execute_reply.started":"2024-04-05T20:09:25.711403Z","shell.execute_reply":"2024-04-05T20:09:36.244518Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stdout","text":"[1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0]\n[1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\nTEST_SIZE = 0.3\nSPLIT_RANDOM_SEED = 42\nMAX_LENGTH = 512\n\ndef encode(examples):\n    result = tokenizer(examples[\"text\"], truncation=True, max_length=MAX_LENGTH, padding=\"max_length\")\n    return result\n\ntokenized_datasets = dataset.map(encode, batched=True, remove_columns=\"text\")","metadata":{"execution":{"iopub.status.busy":"2024-04-05T20:09:36.246770Z","iopub.execute_input":"2024-04-05T20:09:36.247088Z","iopub.status.idle":"2024-04-05T20:11:59.506663Z","shell.execute_reply.started":"2024-04-05T20:09:36.247038Z","shell.execute_reply":"2024-04-05T20:11:59.505757Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/48 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0fe69d3539e4382a84a29abdcbd9241"}},"metadata":{}},{"name":"stdout","text":"CPU times: user 8min 13s, sys: 1.57 s, total: 8min 14s\nWall time: 2min 23s\n","output_type":"stream"}]},{"cell_type":"code","source":"%%time\n\ninput_ids_train, input_ids_v, attention_mask_train, attention_mask_v, label_train, label_v = train_test_split(torch.tensor(tokenized_datasets['input_ids']), \n                                                                                                  torch.tensor(tokenized_datasets['attention_mask']), \n                                                                                                  torch.tensor(tokenized_datasets['target']), \n                                                                                                  random_state=SPLIT_RANDOM_SEED, test_size=TEST_SIZE, shuffle=True)\n\ninput_ids_val, input_ids_test, attention_mask_val, attention_mask_test, label_val, label_test = train_test_split(input_ids_v, attention_mask_v, label_v, \n                                                                                                  random_state=SPLIT_RANDOM_SEED, test_size=0.33, shuffle=True)\n\ntrain_dataset = TensorDataset(input_ids_train, attention_mask_train, label_train)\nval_dataset = TensorDataset(input_ids_val, attention_mask_val, label_val)\ntest_dataset = TensorDataset(input_ids_test, attention_mask_test, label_test)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T20:11:59.508139Z","iopub.execute_input":"2024-04-05T20:11:59.508437Z","iopub.status.idle":"2024-04-05T20:12:49.431088Z","shell.execute_reply.started":"2024-04-05T20:11:59.508411Z","shell.execute_reply":"2024-04-05T20:12:49.430112Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"CPU times: user 49.4 s, sys: 929 ms, total: 50.3 s\nWall time: 49.9 s\n","output_type":"stream"}]},{"cell_type":"code","source":"batch_size = 16\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T20:12:49.432458Z","iopub.execute_input":"2024-04-05T20:12:49.433390Z","iopub.status.idle":"2024-04-05T20:12:49.439534Z","shell.execute_reply.started":"2024-04-05T20:12:49.433350Z","shell.execute_reply":"2024-04-05T20:12:49.438542Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"%%time\n\noptimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=3, eta_min=3e-10)\ntrain(model, optimizer, 3, train_loader, val_loader, batch_size, scheduler, 500, True)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T20:12:49.441613Z","iopub.execute_input":"2024-04-05T20:12:49.442127Z","iopub.status.idle":"2024-04-06T00:15:57.548802Z","shell.execute_reply.started":"2024-04-05T20:12:49.442091Z","shell.execute_reply":"2024-04-06T00:15:57.547747Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mrodion-chernomordin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"wandb version 0.16.6 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.3"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20240405_201249-jrehvyvk</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/jrehvyvk' target=\"_blank\">captain-queen-52</a></strong> to <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/jrehvyvk' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/jrehvyvk</a>"},"metadata":{}},{"name":"stdout","text":"Init loss:\n val loss: 0.8247480937753974, val acc: 0.27775459098497496\n\n","output_type":"stream"},{"name":"stderr","text":"Training 0/3: 100%|██████████| 2084/2084 [1:17:15<00:00,  2.22s/it]   \nTraining 1/3: 100%|██████████| 2084/2084 [1:17:12<00:00,  2.22s/it]   \nTraining 2/3: 100%|██████████| 2084/2084 [1:17:09<00:00,  2.22s/it]   \n","output_type":"stream"},{"name":"stdout","text":"Last batches:\n train loss: 0.38367511574474594, train acc: 0.7272652116402116\n val loss: 0.38769419136797645, val acc: 0.7222454090150251\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">captain-queen-52</strong> at: <a href='https://wandb.ai/rodion-chernomordin/html%20classificator/runs/jrehvyvk' target=\"_blank\">https://wandb.ai/rodion-chernomordin/html%20classificator/runs/jrehvyvk</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20240405_201249-jrehvyvk/logs</code>"},"metadata":{}},{"name":"stdout","text":"CPU times: user 3h 59min 10s, sys: 17min 24s, total: 4h 16min 35s\nWall time: 4h 3min 8s\n","output_type":"stream"}]},{"cell_type":"code","source":"test_loss, test_acc, test_prec, test_rec, test_f1, _, _, _, _ = test(model, test_loader, device, tqdm_desc='Test')\nprint(f'loss = {np.mean(test_loss)}, accuracy = {np.mean(test_acc)}, precision = {np.mean(test_prec)}, recall = {np.mean(test_rec)}, f1_score = {np.mean(test_f1)}')","metadata":{"execution":{"iopub.status.busy":"2024-04-06T00:15:57.549993Z","iopub.execute_input":"2024-04-06T00:15:57.550325Z","iopub.status.idle":"2024-04-06T00:18:22.790942Z","shell.execute_reply.started":"2024-04-06T00:15:57.550298Z","shell.execute_reply":"2024-04-06T00:18:22.789774Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"loss = 0.38188206927251006, accuracy = 0.7281972265023112, precision = 0.0, recall = 0.0, f1_score = 0.0\n","output_type":"stream"}]},{"cell_type":"markdown","source":"[xlm-robert-binclass-trash](https://wandb.ai/rodion-chernomordin/html%20classificator/runs/jrehvyvk?nw=nwuserrodionchernomordin)","metadata":{}},{"cell_type":"markdown","source":"# **Чистка памяти**","metadata":{}},{"cell_type":"code","source":"import gc\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T14:48:52.924785Z","iopub.execute_input":"2024-04-05T14:48:52.925500Z","iopub.status.idle":"2024-04-05T14:48:53.240423Z","shell.execute_reply.started":"2024-04-05T14:48:52.925466Z","shell.execute_reply":"2024-04-05T14:48:53.239359Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}]}]}